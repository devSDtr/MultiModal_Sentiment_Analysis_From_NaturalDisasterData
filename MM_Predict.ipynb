{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ekphrasis"
      ],
      "metadata": {
        "id": "1pQHbMAKN3Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Train Multimodal MLP Models for Sentiment')\n",
        "parser.add_argument('--vtype', type=str, default='clip',\n",
        "                    help='imagenet | places | emotion | clip')\n",
        "parser.add_argument('--ttype', type=str, default='clip',\n",
        "                    help='bertbase | robertabase | clip')\n",
        "parser.add_argument('--mvsa', type=str, default='single',\n",
        "                    help='single | multiple')\n",
        "parser.add_argument('--ht', type=bool, default=True,\n",
        "                    help='True | False')\n",
        "parser.add_argument('--bs', type=int, default=32,\n",
        "                    help='32, 64, 128')\n",
        "parser.add_argument('--epochs', type=int, default=100,\n",
        "                    help='50, 75, 100')\n",
        "parser.add_argument('--lr', type=str, default='2e-5',\n",
        "                    help='1e-4, 5e-5, 2e-5')\n",
        "parser.add_argument('--ftype', type=str, default='feats',\n",
        "                    help='feats | logits')\n",
        "parser.add_argument('--layer', type=str, default='sumavg',\n",
        "                    help='sumavg, 2last, last')\n",
        "parser.add_argument('--norm', type=int, default=1,\n",
        "                    help='0 | 1')\n",
        "parser.add_argument('--split', type=int, default=1,\n",
        "                    help='1-10')\n",
        "parser.add_argument('--smooth', type=bool, default=False,\n",
        "                    help='False | True')\n",
        "parser.add_argument('-f')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "mvsa = args.mvsa\n",
        "batch_size = args.bs\n",
        "normalize = args.norm\n",
        "init_lr = float(args.lr)\n",
        "epochs = args.epochs\n",
        "ftype = args.ftype\n",
        "vtype = args.vtype\n",
        "ttype = args.ttype\n",
        "layer = args.layer\n",
        "split = args.split\n",
        "smooth = args.smooth\n",
        "htag = args.ht"
      ],
      "metadata": {
        "id": "t8usVQb9cynx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn import metrics, preprocessing\n",
        "class MultiMLP_2Mod(nn.Module):\n",
        "    def __init__(self, vdim, tdim):\n",
        "        super(MultiMLP_2Mod, self).__init__()\n",
        "\n",
        "        self.vfc1 = nn.Linear(vdim, 128)#self.vfc1 = nn.Linear(vdim, 128)\n",
        "        self.tfc1 = nn.Linear(tdim, 128)\n",
        "        self.vbn1 = nn.BatchNorm1d(128)\n",
        "        self.tbn1 = nn.BatchNorm1d(128)\n",
        "        self.cf = nn.Linear(256, 3)\n",
        "\n",
        "        self.vdp1 = nn.Dropout(0.5)\n",
        "        self.tdp1 = nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.cf1=nn.Linear(128,3)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.vdp1(self.relu(self.vbn1(self.vfc1(x1))))\n",
        "        x2 = self.tdp1(self.relu(self.tbn1(self.tfc1(x2))))\n",
        "\n",
        "        x = torch.cat((x1,x2), axis=1)\n",
        "\n",
        "        return self.cf(x),self.cf1(x1),self.cf1(x2)"
      ],
      "metadata": {
        "id": "HHD2Vj3bSzBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiDataset2(Dataset):\n",
        "    def __init__(self, vfeats, tfeats, labels, normalize=1):\n",
        "        self.vfeats = vfeats\n",
        "        self.tfeats = tfeats\n",
        "        self.labels = np.array(labels).astype(np.int)\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vfeat = self.vfeats[idx]\n",
        "        tfeat = self.tfeats[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.normalize:\n",
        "            vfeat = preprocessing.normalize(vfeat.reshape(1,-1), axis=1).flatten()\n",
        "            tfeat = preprocessing.normalize(tfeat.reshape(1,-1), axis=1).flatten()\n",
        "\n",
        "        return torch.FloatTensor(vfeat), torch.FloatTensor(tfeat), torch.tensor(label)\n",
        "\n",
        "\n",
        "\n",
        "def get_visual_feats(mvsa, vtype, ftype, htag):\n",
        "    if vtype == 'places':\n",
        "        feats_img = json.load(open('features/places_%s.json'%(mvsa), 'r'))[ftype]\n",
        "        vdim = 2048 if ftype == 'feats' else 365\n",
        "    elif vtype == 'emotion':\n",
        "        feats_img = json.load(open('features/emotion_%s.json'%(mvsa), 'r'))[ftype]\n",
        "        vdim = 2048 if ftype == 'feats' else 8\n",
        "    elif vtype == 'imagenet':\n",
        "        feats_img  = json.load(open('features/imagenet_%s.json'%(mvsa), 'r'))[ftype]\n",
        "        vdim = 2048 if ftype == 'feats' else 1000\n",
        "    elif vtype == 'clip':\n",
        "        feats_img  = json.load(open('/content/imagenet.json', 'r'))['feats']\n",
        "        vdim = 512\n",
        "    else:\n",
        "        feats_img = json.load(open('features/faces_%s.json'%(mvsa),'r'))[ftype]\n",
        "        vdim = 512 if ftype == 'feats' else 7\n",
        "\n",
        "    return np.array(feats_img), vdim"
      ],
      "metadata": {
        "id": "_9qcy6xhhWU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L29w729vuvn"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "import os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "\n",
        "class MMDataset(Dataset):\n",
        "    def __init__(self, dloc, img_transform=None, txt_transform=None, txt_processor=None):\n",
        "        self.file_names = pd.read_csv(os.path.join(dloc,'valid_pairlist.txt'), header=None)\n",
        "        self.dloc = dloc\n",
        "        self.img_transform = img_transform\n",
        "        self.txt_transform = txt_transform\n",
        "        self.txt_processor = txt_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = str(self.file_names.iloc[idx,0])\n",
        "\n",
        "        img = Image.open(os.path.join(self.dloc, 'images', fname+'.jpg')).convert('RGB')\n",
        "        text = open(os.path.join(self.dloc, 'texts', fname+'.txt'), 'r', encoding='utf-8', errors='ignore').read().strip().lower()\n",
        "\n",
        "        if self.img_transform:\n",
        "            img = self.img_transform(img)\n",
        "        else:\n",
        "            img = transforms.ToTensor()(img)\n",
        "\n",
        "        if self.txt_transform:\n",
        "            text = self.txt_transform(text, self.txt_processor)\n",
        "\n",
        "        return img, text\n",
        "\n",
        "\n",
        "def get_text_processor(word_stats='twitter', htag=True):\n",
        "    return TextPreProcessor(\n",
        "            # terms that will be normalized , 'number','money', 'time','date', 'percent' removed from below list\n",
        "            normalize=['url', 'email', 'phone', 'user'],\n",
        "            # terms that will be annotated\n",
        "            annotate={\"hashtag\",\"allcaps\", \"elongated\", \"repeated\",\n",
        "                      'emphasis', 'censored'},\n",
        "            fix_html=True,  # fix HTML tokens\n",
        "\n",
        "            # corpus from which the word statistics are going to be used\n",
        "            # for word segmentation\n",
        "            segmenter=word_stats,\n",
        "\n",
        "            # corpus from which the word statistics are going to be used\n",
        "            # for spell correction\n",
        "            corrector=word_stats,\n",
        "\n",
        "            unpack_hashtags=htag,  # perform word segmentation on hashtags\n",
        "            unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "            spell_correct_elong=True,  # spell correction for elongated words\n",
        "\n",
        "            # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "            # the tokenizer, should take as input a string and return a list of tokens\n",
        "            tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "\n",
        "            # list of dictionaries, for replacing tokens extracted from the text,\n",
        "            # with other expressions. You can pass more than one dictionaries.\n",
        "            dicts=[emoticons]\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "def process_tweet(tweet, text_processor):\n",
        "\n",
        "    proc_tweet = text_processor.pre_process_doc(tweet)\n",
        "\n",
        "    clean_tweet = [word.strip() for word in proc_tweet if not re.search(r\"[^a-z0-9.,\\s]+\", word)]\n",
        "\n",
        "    clean_tweet = [word for word in clean_tweet if word not in ['rt', 'http', 'https', 'htt']]\n",
        "\n",
        "    return \" \".join(clean_tweet)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_bert_embeddings(tweet, model, tokenizer, device):\n",
        "    # Split the sentence into tokens.\n",
        "    input_ids = torch.tensor([tokenizer.encode(tweet, add_special_tokens=True)]).to(device)\n",
        "\n",
        "    # Predict hidden states features for each layer\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            last_out, pooled_out, encoded_layers = model(input_ids, return_dict=False)\n",
        "        except:\n",
        "            last_out, encoded_layers = model(input_ids, return_dict=False)\n",
        "\n",
        "\n",
        "    # Calculate the average of all 22 token vectors.\n",
        "    sent_emb_last = torch.mean(last_out[0], dim=0).cpu().numpy()\n",
        "\n",
        "    # Concatenate the tensors for all layers. We use `stack` here to\n",
        "    # create a new dimension in the tensor.\n",
        "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "\n",
        "    # Remove dimension 1, the \"batches\".\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "    # Swap dimensions 0 and 1.\n",
        "    token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "    # Stores the token vectors, with shape [22 x 3,072]\n",
        "    token_vecs_cat = []\n",
        "\n",
        "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "    # For each token in the sentence...\n",
        "    for token in token_embeddings:\n",
        "\n",
        "        # `token` is a [12 x 768] tensor\n",
        "\n",
        "        # Concatenate the vectors (that is, append them together) from the last\n",
        "        # four layers.\n",
        "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
        "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "\n",
        "        # Use `cat_vec` to represent `token`.\n",
        "        token_vecs_cat.append(cat_vec.cpu().numpy())\n",
        "\n",
        "    sent_word_catavg = np.mean(token_vecs_cat, axis=0)\n",
        "\n",
        "    # Stores the token vectors, with shape [22 x 768]\n",
        "    token_vecs_sum = []\n",
        "\n",
        "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "    # For each token in the sentence...\n",
        "    for token in token_embeddings:\n",
        "\n",
        "        # `token` is a [12 x 768] tensor\n",
        "\n",
        "        # Sum the vectors from the last four layers.\n",
        "        sum_vec = torch.sum(token[-4:], dim=0)\n",
        "\n",
        "        # Use `sum_vec` to represent `token`.\n",
        "        token_vecs_sum.append(sum_vec.cpu().numpy())\n",
        "\n",
        "    sent_word_sumavg = np.mean(token_vecs_sum, axis=0)\n",
        "\n",
        "    # `token_vecs` is a tensor with shape [22 x 768]\n",
        "    token_vecs = encoded_layers[-2][0]\n",
        "\n",
        "    # Calculate the average of all 22 token vectors.\n",
        "    sent_emb_2_last = torch.mean(token_vecs, dim=0).cpu().numpy()\n",
        "\n",
        "    return sent_word_catavg, sent_word_sumavg, sent_emb_2_last, sent_emb_last"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Extract Image and CLIP Features')\n",
        "parser.add_argument('--vtype', type=str, default='imagenet',\n",
        "                    help='imagenet | places | emotion | clip')\n",
        "parser.add_argument('--mvsa', type=str, default='single',\n",
        "                    help='single | multiple')\n",
        "parser.add_argument('--ht', type=bool, default=True,\n",
        "                    help='True | False')\n",
        "parser.add_argument('-f')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "txt_processor = get_text_processor(htag=args.ht)\n",
        "txt_transform = process_tweet\n",
        "\n",
        "def get_resnet_feats():\n",
        "    feats, logits = [], []\n",
        "\n",
        "    def feature_hook(module, input, output):\n",
        "        return feats.extend(output.view(-1,output.shape[1]).data.cpu().numpy().tolist())\n",
        "\n",
        "    if args.vtype == 'imagenet':\n",
        "        print('imgnet')\n",
        "        model = models.__dict__['resnet50'](pretrained=True)\n",
        "    elif args.vtype == 'places':\n",
        "        print('places')\n",
        "        model_file = 'pre_trained/resnet101_places_best.pth.tar'\n",
        "        model = models.__dict__['resnet101'](pretrained=False, num_classes=365)\n",
        "        checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
        "        state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
        "        model.load_state_dict(state_dict)\n",
        "    elif args.vtype == 'emotion':\n",
        "        print('emotion')\n",
        "        model_file = 'pre_trained/best_emo_resnet50.pt'\n",
        "        model = models.__dict__['resnet50'](pretrained=False, num_classes=8)\n",
        "        model.load_state_dict(torch.load(model_file))\n",
        "\n",
        "    model.eval().to(device)\n",
        "\n",
        "    model._modules.get('avgpool').register_forward_hook(feature_hook)\n",
        "\n",
        "    dataset = MMDataset(dloc, img_transforms, txt_transform, txt_processor)\n",
        "    dt_loader = DataLoader(dataset, batch_size=128, sampler=SequentialSampler(dataset))\n",
        "\n",
        "    for i, batch in enumerate(dt_loader):\n",
        "        print(i)\n",
        "\n",
        "        img_inputs = batch[0].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(img_inputs)\n",
        "\n",
        "        logits.extend(outputs.view(-1,outputs.shape[1]).data.cpu().numpy().tolist())\n",
        "\n",
        "\n",
        "    return feats, logits\n",
        "\n",
        "\n",
        "\n",
        "def get_clip_feats():\n",
        "    img_feats, txt_feats = [], []\n",
        "\n",
        "    model, img_preprocess = clip.load('ViT-B/32', device=device)\n",
        "    model.eval()\n",
        "\n",
        "    dataset = MMDataset(dloc, img_transform=img_preprocess, txt_transform=txt_transform, txt_processor=txt_processor)\n",
        "    dt_loader = DataLoader(dataset, batch_size=128, sampler=SequentialSampler(dataset))\n",
        "\n",
        "    for i, batch in enumerate(dt_loader):\n",
        "        print(i)\n",
        "        img_inps, txt_inps = batch[0].to(device), batch[1]\n",
        "\n",
        "        txt_inps = clip.tokenize(txt_inps).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(img_inps)\n",
        "            text_features = model.encode_text(txt_inps)\n",
        "\n",
        "            img_feats.extend(image_features.cpu().numpy().tolist())\n",
        "            txt_feats.extend(text_features.cpu().numpy().tolist())\n",
        "\n",
        "    return img_feats, txt_feats"
      ],
      "metadata": {
        "id": "aRFxUaGHOFms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "X4eZdGfmPN5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dloc='/content/drive/MyDrive/data/mvsa_single'"
      ],
      "metadata": {
        "id": "HpqxK3mYRyxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, RobertaModel, RobertaTokenizer\n",
        "\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Extract BERT Features')\n",
        "parser.add_argument('--btype', type=str, default='robertabase',\n",
        "                    help='bertbase | robertabase')\n",
        "parser.add_argument('--mvsa', type=str, default='single',\n",
        "                    help='single | multiple')\n",
        "parser.add_argument('--ht', type=bool, default=True,\n",
        "                    help='True | False')\n",
        "parser.add_argument('-f')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "txt_processor = get_text_processor(htag=args.ht)\n",
        "txt_transform = process_tweet\n",
        "\n",
        "\n",
        "bert_type = {'bertbase': (BertModel,    BertTokenizer, 'bert-base-uncased'),\n",
        "            'robertabase': (RobertaModel,    RobertaTokenizer, 'roberta-base')}[args.btype]\n",
        "\n",
        "tokenizer = bert_type[1].from_pretrained(bert_type[2])\n",
        "model = bert_type[0].from_pretrained(bert_type[2], output_hidden_states=True)\n",
        "model.to(device).eval()\n",
        "\n",
        "\n",
        "embed_dict = {'catavg':[], 'sumavg': [], '2last': [], 'last': []}\n",
        "\n",
        "ph_data = MMDataset(dloc, txt_transform=txt_transform, txt_processor=txt_processor)\n",
        "ph_loader = DataLoader(ph_data, batch_size=1, sampler=SequentialSampler(ph_data))"
      ],
      "metadata": {
        "id": "e-rToJHTPzI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(ph_loader):\n",
        "\n",
        "    txt_inps = batch[1]\n",
        "\n",
        "    sent_word_catavg, sent_word_sumavg, sent_emb_2_last, sent_emb_last \\\n",
        "        = get_bert_embeddings(txt_inps, model, tokenizer, device)\n",
        "\n",
        "    # embed_dict['catavg'].append(sent_word_catavg.tolist())\n",
        "    embed_dict['sumavg'].append(sent_word_sumavg.tolist())\n",
        "    embed_dict['2last'].append(sent_emb_2_last.tolist())\n",
        "    embed_dict['last'].append(sent_emb_last.tolist())\n",
        "\n",
        "json.dump(embed_dict, open('roberta.json', 'w'))"
      ],
      "metadata": {
        "id": "qWnpAaPFQLvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats, logits = get_resnet_feats()\n",
        "print(np.array(feats).shape, np.array(logits).shape)\n",
        "json.dump({'feats': feats, 'logits': logits}, open('imagenet.json', 'w'))\n"
      ],
      "metadata": {
        "id": "BefekTXASaSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vdim=2048\n",
        "tdim=768\n",
        "model_vi = MultiMLP_2Mod(vdim, tdim)\n",
        "\n",
        "model_vi.to(device)\n",
        "model_vi.load_state_dict(torch.load(\"/content/mmm.pt\"))"
      ],
      "metadata": {
        "id": "Hvb5ecIwZXH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_df = pd.read_csv(dloc+'/valid_pairlist.txt', header=None)\n",
        "all_labels = pair_df[1].to_numpy().flatten()\n",
        "te_ids=[0]\n",
        "lab_test = all_labels[te_ids]"
      ],
      "metadata": {
        "id": "HgweKykbZlAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats_text = json.load(open('/content/roberta.json','r'))\n",
        "feats_text = feats_text[layer]\n",
        "tdim = 3072 if 'catavg' in layer else 768\n",
        "feats_text = np.array(feats_text)\n",
        "ft_te_txt = feats_text[te_ids]"
      ],
      "metadata": {
        "id": "hMW1MnO6ceXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats_img, vdim = get_visual_feats(mvsa, vtype, ftype, htag)\n",
        "ft_te_img = feats_img[te_ids]"
      ],
      "metadata": {
        "id": "SC4SJ2fqc0qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "te_data = MultiDataset2(ft_te_img, ft_te_txt, lab_test, normalize)"
      ],
      "metadata": {
        "id": "nXoCUm1bheQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "te_loader = DataLoader(dataset=te_data, batch_size=1, num_workers=2)"
      ],
      "metadata": {
        "id": "h9EZpqn0hxgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vi.eval()\n",
        "count=0\n",
        "true=0\n",
        "false=0\n",
        "for inputs1, inputs2, labels in te_loader:\n",
        "  inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
        "\n",
        "  outputs,outputs1,outputs2 = model_vi(inputs1, inputs2)\n",
        "\n",
        "  preds = torch.argmax(outputs.data, 1)\n",
        "  preds1=torch.argmax(outputs1.data,1)\n",
        "  preds2=torch.argmax(outputs2.data,1)\n",
        "\n",
        "  print(\"ground truth:  \",labels.cpu().numpy())\n",
        "\n",
        "  #print(te_ids[count])\n",
        "  print(\"MM output:     \",preds.cpu().numpy())\n",
        "  print(\"Image Feature: \",preds1.cpu().numpy())\n",
        "  print(\"Text Feature:  \",preds2.cpu().numpy())\n",
        "  if labels==preds:\n",
        "    true=true+1\n",
        "\n",
        "  else:\n",
        "    false=false+1\n",
        "\n",
        "  print(\" \")\n",
        "  count=count+1"
      ],
      "metadata": {
        "id": "WPXfXfi5h6gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_extractions(dloc):\n",
        "  bert_type = {'bertbase': (BertModel,    BertTokenizer, 'bert-base-uncased'),\n",
        "            'robertabase': (RobertaModel,    RobertaTokenizer, 'roberta-base')}[args.btype]\n",
        "\n",
        "  tokenizer = bert_type[1].from_pretrained(bert_type[2])\n",
        "  model = bert_type[0].from_pretrained(bert_type[2], output_hidden_states=True)\n",
        "  model.to(device).eval()\n",
        "\n",
        "\n",
        "  embed_dict = {'catavg':[], 'sumavg': [], '2last': [], 'last': []}\n",
        "  txt_processor = get_text_processor(htag=args.ht)\n",
        "  txt_transform = process_tweet\n",
        "  ph_data = MMDataset(dloc, txt_transform=txt_transform, txt_processor=txt_processor)\n",
        "  ph_loader = DataLoader(ph_data, batch_size=1, sampler=SequentialSampler(ph_data))\n",
        "  for i, batch in enumerate(ph_loader):\n",
        "    print(i)\n",
        "    txt_inps = batch[1]\n",
        "\n",
        "    sent_word_catavg, sent_word_sumavg, sent_emb_2_last, sent_emb_last \\\n",
        "        = get_bert_embeddings(txt_inps, model, tokenizer, device)\n",
        "\n",
        "    # embed_dict['catavg'].append(sent_word_catavg.tolist())\n",
        "    embed_dict['sumavg'].append(sent_word_sumavg.tolist())\n",
        "    embed_dict['2last'].append(sent_emb_2_last.tolist())\n",
        "    embed_dict['last'].append(sent_emb_last.tolist())\n",
        "\n",
        "    json.dump(embed_dict, open('roberta1.json', 'w'))\n",
        "\n",
        "    #visual extract\n",
        "    feats, logits = get_resnet_feats()\n",
        "    print(np.array(feats).shape, np.array(logits).shape)\n",
        "    json.dump({'feats': feats, 'logits': logits}, open('imagenet.json', 'w'))\n",
        ""
      ],
      "metadata": {
        "id": "rWcBpgR9h_bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_extractions(dloc)"
      ],
      "metadata": {
        "id": "Avm5j63Dnr2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loaders(dloc):\n",
        "  pair_df = pd.read_csv(dloc+'/valid_pairlist.txt', header=None)\n",
        "  all_labels = pair_df[1].to_numpy().flatten()\n",
        "  te_ids=[0]\n",
        "  lab_test = all_labels[te_ids]\n",
        "  feats_text = json.load(open('/content/roberta1.json','r'))\n",
        "  feats_text = feats_text[layer]\n",
        "  tdim = 3072 if 'catavg' in layer else 768\n",
        "  feats_text = np.array(feats_text)\n",
        "  ft_te_txt = feats_text[te_ids]\n",
        "  feats_img, vdim = get_visual_feats(mvsa, vtype, ftype, htag)\n",
        "  ft_te_img = feats_img[te_ids]\n",
        "  te_data = MultiDataset2(ft_te_img, ft_te_txt, lab_test, normalize)\n",
        "  te_loader = DataLoader(dataset=te_data, batch_size=1, num_workers=2)\n",
        "\n",
        "  return te_loader"
      ],
      "metadata": {
        "id": "4giXashunv9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dloc='/content/drive/MyDrive/data/mvsa_single'  #folder path for data"
      ],
      "metadata": {
        "id": "7BhNd0mIt88w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "te_loader=get_loaders(dloc)"
      ],
      "metadata": {
        "id": "Z4g1cq7Epwv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path=\"/content/mmm.pt\" #Pretrained model file"
      ],
      "metadata": {
        "id": "w_gIim1KqI5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model_path):\n",
        "  vdim=2048\n",
        "  tdim=768\n",
        "  model_vi = MultiMLP_2Mod(vdim, tdim)\n",
        "\n",
        "  model_vi.to(device)\n",
        "  model_vi.load_state_dict(torch.load(model_path))\n",
        "\n",
        "  return model_vi\n"
      ],
      "metadata": {
        "id": "zihH4bIsp3ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vi=get_model(model_path)"
      ],
      "metadata": {
        "id": "o6di73wsqVpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results(te_loader):\n",
        "  model_vi.eval()\n",
        "  for inputs1, inputs2, labels in te_loader:\n",
        "    inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
        "\n",
        "    outputs,outputs1,outputs2 = model_vi(inputs1, inputs2)\n",
        "\n",
        "    preds = torch.argmax(outputs.data, 1)\n",
        "    preds1=torch.argmax(outputs1.data,1)\n",
        "    preds2=torch.argmax(outputs2.data,1)\n",
        "    file_path=dloc+\"/valid_pairlist.txt\"\n",
        "    with open(file_path, 'r') as file:\n",
        "      data = file.read().split(',')\n",
        "      num_array = np.array(data, dtype=int)\n",
        "\n",
        "    print(\"ground truth text feature  : \",num_array[2])\n",
        "    print(\"ground truth image feature : \",num_array[3])\n",
        "    print(\"ground truth:  \",labels.cpu().numpy())\n",
        "    print(\"MM output:     \",preds.cpu().numpy())\n",
        "    print(\"Image Feature: \",preds1.cpu().numpy())\n",
        "    print(\"Text Feature:  \",preds2.cpu().numpy())\n",
        "\n"
      ],
      "metadata": {
        "id": "-PCRZOF9qg4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_results(te_loader)"
      ],
      "metadata": {
        "id": "h6gbC85lsVv_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}